{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a714c152",
   "metadata": {},
   "source": [
    "### What are Tokenizers\n",
    "\n",
    "- Converts raw text[***human readable***] to nummerical type[***neural network processable language***]\n",
    "\n",
    "- tokenizer does following tasks\n",
    "\n",
    "    * build vocabulary, if one does not exists\n",
    "    * assign tokenIDs[indices] to given word corpus[tokens]\n",
    "    * reassign tokenIDs to tokens\n",
    "\n",
    "- A preprocessing step in tranformer model training & inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b8a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as cots\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tiny_shakespeare import TinyShakespeare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fccae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "train: 1003854 characters\n",
      "\n",
      "{'text': Value('string')}\n",
      "validation: 55770 characters\n",
      "\n",
      "{'text': Value('string')}\n",
      "test: 55770 characters\n",
      "\n",
      "{'text': Value('string')}\n",
      "First Citizen:\n",
      "Before we proceed any further, hear\n"
     ]
    }
   ],
   "source": [
    "shakespeare = TinyShakespeare()\n",
    "shakespeare.download_and_prepare()\n",
    "dataset = shakespeare.as_dataset()\n",
    "print(dataset)\n",
    "\n",
    "for split in dataset:\n",
    "    print(f\"{split}: {len(dataset[split]['text'][0])} characters\\n\")\n",
    "    print(dataset[split].features)\n",
    "\n",
    "train = dataset[\"train\"][\"text\"][0]\n",
    "val = dataset[\"validation\"][\"text\"][0]\n",
    "test = dataset[\"test\"][\"text\"][0]\n",
    "\n",
    "print(train[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7bf24",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "### 1. Bag of Words: Word Based\n",
    "* Whitespace/Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589ad462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12559\n",
      "First 10 words: ['First', 'Citizen', 'Before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n"
     ]
    }
   ],
   "source": [
    "# Split on punctuation and whitespace\n",
    "reg_vocab = re.split(r\"[ \\t\\n\\r\\f\\v!\\\"#$%&'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]+\", train)\n",
    "\n",
    "# Remove empty tokens (if any)\n",
    "tokens = [t for t in reg_vocab if t]\n",
    "print(f\"Vocabulary size: {len(set(tokens))}\")\n",
    "print(f\"First 10 words: {tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736299bf",
   "metadata": {},
   "source": [
    "### 2. Character Based\n",
    "* Byte Array\n",
    "\n",
    "To convert any text dataset to its character based(0-256, ASCII) tokens in python, follow below steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65b9076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b'Hello World')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to byte array\n",
    "# This is a simple way to convert text to byte array in python\n",
    "# It will convert each character to its corresponding byte value\n",
    "text = \"Hello World\" # We can use above 'train' set as well\n",
    "b_arr = bytearray(text, \"utf-8\")\n",
    "b_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa116382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When we save this byte array in an Python list, it will be stored as corresponding char-bytes\n",
    "list(b_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e996f9",
   "metadata": {},
   "source": [
    "#### Example: With Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95d1ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1003854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[70, 105, 114, 115, 116, 32, 67, 105, 116, 105]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_char_tokens = list(bytearray(train, \"utf-8\"))\n",
    "print(f\"size: {len(train_char_tokens)}\")\n",
    "train_char_tokens[:10]  # Display first 10 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468c598",
   "metadata": {},
   "source": [
    "### 3. Sub Word Based\n",
    "\n",
    "#### Byte Pair Encoding\n",
    "\n",
    "Pseudocode:\n",
    "* Task-A:\n",
    "    1. split char_corpus(***text data used to prepare vocabulary***) into characters\n",
    "    2. add unique chars as (index, char) pair to vocabulary_table\n",
    "    3. for i in ***K***(hyper_parameter):\n",
    "        * merge consequence of chars as pair\n",
    "        * count the frequency of pairs  - use an (pair, freq) dict\n",
    "        * pick a high freq pair\n",
    "        * update the char_corpus\n",
    "        * update vocabulary_table\n",
    "        * repeat untill K times \n",
    "* Task-B: \n",
    "   - ***encode*** the char_corpus using vocabulary_table -> token IDs(index)\n",
    "* Task-C: \n",
    "   - if needed - ***decode*** in the same way to see the bytes pairs of given text example> vocabulary_table helps\n",
    "\n",
    "### Sources:\n",
    "\n",
    "* [pseudocode](https://sebastianraschka.com/blog/2025/bpe-from-scratch.html)\n",
    "* [get_into_implementation_details](https://www.youtube.com/watch?v=20xtCxAAkFw)\n",
    "\n",
    "* [tiktoken@OpenAI - App](https://tiktokenizer.vercel.app/?model=gpt2)\n",
    "* [tiktoken@OpenAI - Github Repo](https://github.com/openai/tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13f5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import heapq\n",
    "# import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71d6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_char_corpus(num_chars, most_repeated, vocabulary_table):\n",
    "    copie=np.copy(num_chars).tolist()  # Create a copy of the original characters\n",
    "    # copie\n",
    "    for i in range(len(copie)-1):\n",
    "        if i < len(copie)-1 and (copie[i]+copie[i+1] == most_repeated):\n",
    "                copie[i] = list(vocabulary_table.values())[-1]\n",
    "                del copie[i+1]\n",
    "\n",
    "    # for i, j in zip(num_chars, copie):\n",
    "    #     print(f\"{i} -> {j}\")\n",
    "    \n",
    "    return copie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2011fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_code_encoding(text_string, num_steps=100):\n",
    "    char_corpus = np.array(list(text_string))                                                # Step1> define the test string\n",
    "  \n",
    "    unique_chars = np.unique(char_corpus)                                                    # Step2> get unique characters and index them in vocabulary_table\n",
    "    # Create vocabulary table\n",
    "    vocabulary_table = {i: char for i, char in enumerate(unique_chars.tolist())}  \n",
    "   \n",
    "    for i in range(num_steps):                                                               #                       ----------------------------------------------------------------\n",
    "                                                                                                                                                                                   #|\n",
    "        consec_char_pairs = zip(char_corpus, char_corpus[1:])                                # Step3> create pairs of consecutive characters                                       #|\n",
    "        char_pairs_arr = np.array(list(consec_char_pairs))                                                                                                                         #|\n",
    "                                                                                                                                                                                   #|\n",
    "        # char_pairs_merged = np.apply_along_axis(lambda x: x[0] + x[1], 1, char_pairs_arr)                                                                                        #|\n",
    "        # char_pairs_merged = np.apply_along_axis(lambda x: ''.join(x), 1, char_pairs_arr)                                                                                         #|  \n",
    "        char_pairs_merged = np.char.add(char_pairs_arr[:, 0], char_pairs_arr[:, 1])          # Step4> chars pairs merging                                                          #|\n",
    "                                                                                                                                                                                   #|\n",
    "        unique_pairs = np.array(np.unique(char_pairs_merged, return_counts=True))            # Step5> counts each pair frequency                                                   #|\n",
    "                                                                                                                                                                                   #|\n",
    "        merged=unique_pairs[0]                                                               # Step6> make an freq-pair array out of the numpy counter func                        #|\n",
    "        counts=unique_pairs[1]                                                                                                                                                     #|\n",
    "                                                                                                                                                                                   #|\n",
    "        pairs = zip(counts, merged)                                                                                                                                                #|\n",
    "        freq_pair_dict = np.array(list(pairs))                                                                                                                              \n",
    "        \n",
    "        \n",
    "        only_freqs = np.array(freq_pair_dict[:,0], dtype=int)                                # Step7> get highest freq-pair\n",
    "        high_freq_pair=freq_pair_dict[np.where(only_freqs == np.max(only_freqs))]\n",
    "        \n",
    "        most_repeated = str(high_freq_pair[0][1])\n",
    "        \n",
    "        vocabulary_table.update({(list(vocabulary_table.keys())[-1]+1, most_repeated)})      # Step8> update vocabulary table with the most repeated pair\n",
    "\n",
    "        char_corpus = np.array(update_char_corpus(char_corpus, most_repeated, vocabulary_table))  # Step9> update the character corpus\n",
    "\n",
    "    return vocabulary_table, char_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7204b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d7a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b73a9bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Corpus: ['First ' 'Citizen:\\n' 'B' 'e' 'for' 'e ' 'we ' 'pro' 'ce' 'ed ']\n",
      "Vocabulary Table: {0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: ',', 5: '-', 6: '.', 7: ':', 8: ';', 9: '?'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# test_string = \"Hello World longer text to test the tokenizer functionality\"\n",
    "test_string=train[:50000]\n",
    "vocabulary_table, char_corpus = byte_code_encoding(test_string, num_steps=500)\n",
    "print(\"Character Corpus:\", char_corpus[:10])\n",
    "print(\"Vocabulary Table:\", dict(islice(vocabulary_table.items(), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da3cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
