{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f76ca9",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544326b",
   "metadata": {},
   "source": [
    "### Embeddings in the World of NLP[Natural Language Processing]\n",
    "\n",
    "1. *what are embeddings?*\n",
    "* embeddings are the set of nummericals(vectors aka) that represent the human language in the computer world.\n",
    "2. *why we need such an representation?. are not there any other ways?*\n",
    "* because just the nummericals alone do not do any job. we need certain kind of ways(algorithms), a computer can extract useful information and process it and give the results that are relavent to problem domain. and embeddings is an umbella term. since decades researchers have been working on to find the right algorithm that proceducess right nummericals that computer can finally understand the human language as human do. now(2025) we have one, that is ***Transformer Models***. \n",
    "3. *history of algorithms in embeddings*\n",
    "- **statistical word embedding** - era of begining\n",
    "- **static word embedding** - era of improvement\n",
    "- **dynamic or contextual word embedding** - era of achievement \n",
    "\n",
    "4. *Sources:*\n",
    "- [Overview of Embeddings: Good Start](https://huggingface.co/spaces/hesamation/primer-llm-embedding?section=what_are_embeddings?)\n",
    "- [Google_Embedding_Projector](https://projector.tensorflow.org/)\n",
    "- [Github-Helper-Repo](https://github.com/hesamsheikh/llm-mechanics/blob/main/notebooks/embedding/embedding_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e2a9a",
   "metadata": {},
   "source": [
    "#### Statistical Word Embedding\n",
    "### **tf-idf**\n",
    "this algorithm is mainly running around two concepts\n",
    "- importance of a word w in a document d\n",
    "\\begin{align}\n",
    "TF(w,d) &= \\frac{number \\; of \\; times \\; a \\; word \\; w \\; appears \\; in \\; a \\; document \\; d}{total \\; number \\; of \\; words \\; in \\; document \\; d}\n",
    "\\end{align}\n",
    "        \n",
    "- is this an special word in document corpus\n",
    "\\begin{align}\n",
    "IDF(w,S) &= \\frac{total \\; number \\; of \\; documents \\; in \\; dataset \\; S}{number \\; of \\; documents \\; contain \\; word \\; w}\n",
    "\\end{align}\n",
    "+ Pseudo code:\n",
    "\n",
    "*  calculate the TF and IDF of each word in an corpus\n",
    "*  these act as vector embeddings to compare the relavence and importance of an word in given word corpus\n",
    "\n",
    "+ Sources\n",
    "* [Good Source](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec1199",
   "metadata": {},
   "source": [
    "# For code implementation visit 1_tf_idf.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e508fee",
   "metadata": {},
   "source": [
    "#### Static Word Embedding\n",
    "### **word2vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e5ab3",
   "metadata": {},
   "source": [
    "# For code implementation visit 2_word2vec_skipgram.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ec206",
   "metadata": {},
   "source": [
    "#### Contextual Word Embedding\n",
    "### **transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c6cc6",
   "metadata": {},
   "source": [
    "# For code implementation visit 3_transformer_embedding.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
